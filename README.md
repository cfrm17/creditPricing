# Credit Pricing Model Calibration


A calibration procedures of the Default Correlation model is presented. There are two principal modifications.  The first is to change the manner in which asset correlations are converted into default correlations, the second is a small change in the algorithm by which the probability equations of the model are solved.  These changes are considered appropriate, and are necessary for the model to be considered robust enough to underpin the structuring and trading of complex credit contingent instruments.

The first modification involves the conversion of asset correlations into default correlations. Conversion of asset correlations into default correlations in the original model is carried out by equating the joint default probability of the bivariate normal copula between two names at a specific time horizon, i.e., if   is the joint default probability for time horizon   with asset correlation  and   is the joint probability of default for this model with default correlation   then given that

  (1)
and 
  (2)

where
  (3)

we can solve for 
 
where
 . (4)

Previously, this conversion was carried out at a fixed time horizon   even though the credit curves    can vary with time.  Default correlations consistent with the model must lie in the range  , where 

  (5)

Default correlations generated by this conversion procedure can easily  lie outside the allowed range for  .

The modified conversion procedure simply re-converts the (constant) asset correlation into a default correlation each time there is a change in the value of one of the credit curves.  This assures that the default correlations are always within the allowed range.  It also has the benefit of ensuring that the asset correlation is held constant over time.

The second modification involves a modification of the procedure used for solving the probability equations of the model.  The solution to these equations can be written

  (6)

and in the original model, any computed  , or   or   caused a fatal error.  The new procedure for computing according to equation (6)  has been dubbed the “ostrich” algorithm.  This is so named because we will simply pretend that the problem does not exist.  In the course of constructing the triangular solution using equation (6), we adjust the results as follows:

	If   then  .
	If   then  
	If   then  

Then continue constructing the solution using the modified values for the remainder of the computation.

This simple procedure has the effect of normalizing the solution, allowing the solution to recover.  This modification is accompanied by an error-checking condition.  If the solution obtained by the ostrich algorithm does not match the inputs to within a specified tolerance, a failure is reported.

The testing process was carried out in two stages.  First, the independent implementation of the model which was constructed for the testing of the original model outlined in was modified. 

The second phase of the process was carried out using the previous version of the model independently modified to conform to the new specifications by the author to make further tests.  These tests were conducted with both the new implementation of the model and the previous version of the model.

In order to further investigate the differences between the modified model and the original model, we have constructed our own modified version of the model.  This version is equipped with an error reporting facility, which reports the error between the inputs and the recomputation of those inputs from the solution to the probability equations.   The error is computed as the Frobenius norm of the differences in the inputs and the inputs as reconstructed from the solution as given by equation (6) with the ostrich algorithm.

Several test cases were developed.  All involved the pricing of a first to default basket of ten names (see https://finpricing.com/lib/EqBarrier.html.  The ten credit curves were all created from the following simple formula.  The credit spread   from which the credit curves are built are created from the formula


  (7)

thus  is an overall multiplicative factor, and   is a slope parameter.    Changing   and   thus allows us to create a wide variety of curves from a single set of   values.  

The inputs to the model consist of a set of possibly time dependent hazard rates   which can be considered the unconditional intensities of Poisson processes which govern the arrival of default events which involve the  th defaultable entity, which we will refer to as names.  Thus we can write that

			  (1)

where the notation   indicates that the sum is to be taken over all subsets   of   which contain the name  .  For the moment we leave   unspecified.

We can also define in a similar manner a quantity   which is the unconditional intensity of a Poisson process which governs the arrival rate of default events involving simultaneous default of name   and name  , given as

			  (2)

where in this case the notation   indicates that the sum is to be taken over all subsets   of   which include both name   and name  .

A third quantity which we will define is   , which is the unconditional intensity of a Poisson process which governs the arrival rate of default events involving name  , name   or simultaneously names   and  .  This will be given as

		  (3)

where the notation   indicates summation over all subsets   of   which include name   but do not include name  .  Clearly   can be rewritten

		  (4)

or more simply

			 . (4b)

We are now in a position to define default correlation as the probability for both name   and name   to default if either of them defaults, which will be given by the conditional probability

			  (5)

or

			 . (6)

This definition of   then implies that

			 .  (7)

Given this definition of default correlation  , the   are then given in terms of the inputs  , and the  ,  and we have a system of equations which we can then solve for the  , providing a means to model credit contingent structures.

Working directly in terms of the intensities of the Poisson processes for the default of all subsets   of   entities may be unwieldy.  Therefore we seek to find a way to reduce the complexity of the problem in some way.  This can be done by introducing the idea of primitive and joint events.
We therefore define a set of primitive event arrival rates   and a set of joint event arrival rates  , which are related to   as follows:

		  (8)

which determines the interpretation that   is the intensity of a Poisson process that governs the arrival rate of default events involving name  , and that   is the probability of simultaneous default of name   conditional on name   not having already defaulted.

The introduction of  ,   according to equation (8) can be shown to lead to the following equations which we must solve:
			
			  (9)
and 
		 			  (10)

where  , and where the   and  the  are the inputs to the model.  
We must solve this system of equations subject to the constraints that   and   for  .  Equation (9) provides   constraints.   From the structure of equation (10), we see that the symmetry of   does not by itself impose any constraints on the  , and thus we have an additional   constraints.

On the other hand, we are searching for   unknown quantities  , and   unknowns  .  Therefore , we have   equations and   unknowns, and therefore thesystem of equations is underdetermined.  In order to have the number of equations equal the number of unknowns, we find that we must impose an additional   constraints.

In the existing model, the “triangular” ansatz, setting   for   is made, which permits solving for the   and the   explicitly. The triangular ansatz imposes the additional   constraints by setting   for  .  Note, however, that in the preceding analysis we have not taken into account the built-in constraints on the ranges of the solution.  These represent an additional   constraints from   for  . The fact that  , can be seen to follow from equation (9) when the ranges constraints on    are applied, which leaves us to impose an additional   constraints that  .  

On the face of it, it might seem that we are free to impose any default correlation we choose, and if we had defined default correlation in a manner analogous to statistical correlation, that would no doubt be the case.  However, the assumptions and definitions of the present model will not permit this, as we will see.  Recall from equation (6) the definition of default correlation  :

			  

Suppose now that we have two names   and    with their respective hazard rates.  Assume also that  , and let us write   for some  .  Now, we can approximate   and then we have

		 . (6b)

We can see then that if the specified inputs are inconsistent with this limit, which is a consequence of the definition of default correlation in this model, we will not be able to find an appropriate solution to equations (9) and (10).

It is thus important therefore, when calibrating the model to maintain the best possible consistency across time, with the possibility that the maximal default correlation between two curves can vary  with time.  
